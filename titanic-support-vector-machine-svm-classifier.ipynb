{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<img src=\"https://github.com/datastrider/titanic_svm/blob/main/titanic_comp_pic.jpg?raw=true\" ></img>\n# <p style=\"background-color:#0d101c;font-family:arial;color:#ffffff;font-size:150%;text-align:center;border-radius:20px;\">Table of Contents</p>\n\n[Project Motivation](#project_motivation)\n_________________\n1. [Import Modules](#import_modules)\n2. [Load Data](#load_data)\n3. [Data Exploration](#data_exploration)<br>\n4. [Data Cleaning & Preprocessing](#data_preprocessing)<br>\n    4.1 [Checking Missing Values](#check_missing_values)<br>\n    4.2 [Checking Unusual/Invalid Values](#unusual_values)<br>\n    4.3 [Encoding Categorical Data](#encode_data)<br>\n    4.4 [Removing Columns/Attributes](#remove_cols)<br>\n5. [Model Training: No Sklearn](#no_sklearn)\n6. [Model Training: Sklearn](#sklearn)\n7. [Kaggle Submission](#kaggle_sub)<br>\n    7.1 [Cleaning and Processing Test Data](#clean_test_data)<br>\n    7.2 [Create sumbission csv](#create_submission_csv)<br>","metadata":{}},{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"project_motivation\"></a>\n# <p style=\"background-color:#0d101c;font-family:arial;color:#ffffff;font-size:150%;text-align:center;border-radius:20px;\">Project Motivation</p>\n\nThe motivation of this project was to practice and improve my methodologies used when investigating a dataset, cleaning and preprocessing it, and then using it with a machine learning algorithm.\n\nIt was also an opportunity to learn more about a specific machine learning algorithm: <b>Support Vector Machine</b><br>\n\nHere I try to implement the base algorithm <b>from scratch</b> (no hyper-parameter optimisation), using gradient descent to calculate the weights and bias of the algorithm, and then use and optimise the <b>*Sklearn* implementation</b> to see how it compares, and what results can be achieved after fitting on the full training dataset, and then making predictions off of the test dataset and checking against Kaggle's answers.\n\nThe results obtained after uploading to Kaggle are not the best (about 0.66), but I enjoyed this project nontheless.","metadata":{}},{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"import_modules\"></a>\n# <p style=\"background-color:#0d101c;font-family:arial;color:#ffffff;font-size:150%;text-align:center;border-radius:20px;\">Import Modules</p>\n","metadata":{}},{"cell_type":"code","source":"import time\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom skopt import BayesSearchCV","metadata":{"execution":{"iopub.status.busy":"2022-01-15T19:19:24.553706Z","iopub.execute_input":"2022-01-15T19:19:24.554379Z","iopub.status.idle":"2022-01-15T19:19:26.080772Z","shell.execute_reply.started":"2022-01-15T19:19:24.554326Z","shell.execute_reply":"2022-01-15T19:19:26.079891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"load_data\"></a>\n# <p style=\"background-color:#0d101c;font-family:arial;color:#ffffff;font-size:150%;text-align:center;border-radius:20px;\">Load Data</p>","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-01-15T19:19:26.082343Z","iopub.execute_input":"2022-01-15T19:19:26.08256Z","iopub.status.idle":"2022-01-15T19:19:26.114732Z","shell.execute_reply.started":"2022-01-15T19:19:26.082533Z","shell.execute_reply":"2022-01-15T19:19:26.113866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head(n=10)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T19:19:26.116049Z","iopub.execute_input":"2022-01-15T19:19:26.116338Z","iopub.status.idle":"2022-01-15T19:19:26.144718Z","shell.execute_reply.started":"2022-01-15T19:19:26.1163Z","shell.execute_reply":"2022-01-15T19:19:26.144153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"data_exploration\"></a>\n# <p style=\"background-color:#0d101c;font-family:arial;color:#ffffff;font-size:150%;text-align:center;border-radius:20px;\">Data Exploration</p>\n\nIn this section, we will get a quick overview of some basic statistics of the dataset, and then look at how different attributes relate to who survived or not.","metadata":{}},{"cell_type":"code","source":"train_data.describe()","metadata":{"execution":{"iopub.status.busy":"2022-01-15T19:19:26.146964Z","iopub.execute_input":"2022-01-15T19:19:26.147194Z","iopub.status.idle":"2022-01-15T19:19:26.187139Z","shell.execute_reply.started":"2022-01-15T19:19:26.147165Z","shell.execute_reply":"2022-01-15T19:19:26.186487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.pivot_table(train_data, index=[\"Survived\"])","metadata":{"execution":{"iopub.status.busy":"2022-01-15T19:19:26.188268Z","iopub.execute_input":"2022-01-15T19:19:26.188671Z","iopub.status.idle":"2022-01-15T19:19:26.21571Z","shell.execute_reply.started":"2022-01-15T19:19:26.188624Z","shell.execute_reply":"2022-01-15T19:19:26.215143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>How does Age and Fare correlate with survivability</b>\n\nBelow, we can see that there was not a large disparity of the survival rate of different ages. The median age of passengers that survived and those that did not were the same. There was a difference spotted with the fare price, where those who paid more tended to be more likely to survive.","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(1,2)\nfig.set_figwidth(25)\nfig.set_figheight(8)\n\ncheck_cols = [\"Age\", \"Fare\"]\nsns.set_style(\"dark\")\nfor i in range(len(check_cols)):\n    \n    sns.kdeplot(data=train_data.loc[train_data[\"Survived\"] == 1, check_cols[i]],\n                  ax=axes[i],\n                  label=\"Survived\",\n                  color='blue',\n                  shade=True)\n\n    sns.kdeplot(data=train_data.loc[train_data[\"Survived\"] == 0, check_cols[i]],\n                  ax=axes[i],\n                  label=\"Did not survive\",\n                  color='red',\n                  shade=True)\n\n    # plot vertical lines\n    axes[i].axvline(train_data.loc[train_data[\"Survived\"] == 1, check_cols[i]].median(),\n                   color='blue')\n\n    axes[i].axvline(train_data.loc[train_data[\"Survived\"] == 0, check_cols[i]].median(),\n                   color='red')\n    \n    # plot annotations of values corresponding to the vertical lines\n    axes[i].annotate(\"Median {} Survived: {}\".format(check_cols[i],\n                                                        train_data.loc[train_data[\"Survived\"] == 1, check_cols[i]].median()),\n                                                        xy=(0.45, 0.95),\n                                                        xycoords='axes fraction',\n                                                        fontsize=15)\n    \n    axes[i].annotate(\"Median {} Not Survived: {}\".format(check_cols[i],\n                                                         train_data.loc[train_data[\"Survived\"] == 0, check_cols[i]].median()),\n                                                         xy=(0.45, 0.90),\n                                                         xycoords='axes fraction',\n                                                         fontsize=15)\n    \n    axes[i].title.set_text(\"{} and Survival\".format(check_cols[i]))\n    axes[i].legend()\nplt.show()\nplt.close()","metadata":{"execution":{"iopub.status.busy":"2022-01-15T19:19:26.216867Z","iopub.execute_input":"2022-01-15T19:19:26.217238Z","iopub.status.idle":"2022-01-15T19:19:26.82744Z","shell.execute_reply.started":"2022-01-15T19:19:26.217195Z","shell.execute_reply":"2022-01-15T19:19:26.826628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>How does Pclass and Sex correlate with survivability</b>\n\nWe can see that those of a lower socio-economic status (Pclass) were more likely to have died than those of a higher class. This could potentially caused by people of higher class being let on life-boats, and/or people of higher socio-economic class were able to spend more. It was shown earlier that those who paid a higher fare were also more likely to have survived than those who paid a lower fare.\n\n<b>It is clear that *'Age'*, *'Fare'*, *'Pclass'* and *'Sex'* are important attributes to consider when deciding on whether a passenger was likely to have survived or not </b>","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(1,2)\nfig.set_figwidth(15)\nfig.set_figheight(5)\nsns.countplot(x=\"Pclass\", hue=\"Survived\", data=train_data, palette=[\"#F34D4D\", \"#2B72D9\"], ax=axes[0])\naxes[0].set_xticks([1,2,3])\nsns.countplot(x=\"Sex\", hue=\"Survived\", data=train_data, palette=[\"#F34D4D\", \"#2B72D9\"], ax=axes[1])\naxes[0].set_title(\"Pclass and Survival\")\naxes[1].set_title(\"Sex and Survival\")\nplt.show()\nplt.close()","metadata":{"execution":{"iopub.status.busy":"2022-01-15T19:19:26.828524Z","iopub.execute_input":"2022-01-15T19:19:26.828761Z","iopub.status.idle":"2022-01-15T19:19:27.19673Z","shell.execute_reply.started":"2022-01-15T19:19:26.828723Z","shell.execute_reply":"2022-01-15T19:19:27.195795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"data_preprocessing\"></a>\n# <p style=\"background-color:#0d101c;font-family:arial;color:#ffffff;font-size:150%;text-align:center;border-radius:20px;\">Data Cleaning & Preprocessing</p>\n\n<b>Steps involved in Data Cleaning and Preprocessing</b>\n\n* <b>Look for missing values and correct them</b>\n    1. Delete observations with missing categorical data\n    2. Replace missing numeric data with the median of that particular attribute\n    <br><br>\n\n* <b>Look for unusual values and correct them</b>\n    1. Variations of the same text categorical data e.g. 'Male\" and 'male'\n    2. Invalid values of numeric data e.g Age < 0\n    3. Strong outlier values of numeric data\n    <br><br>\n\n* <b>Encode categorical data using the One-Hot encoder</b>\n<br>\n    Creates a column for each categorical value, and populates them with a 1 or 0\n    e.g. \n\n\n\n|Embarked|\n|--|\n|S|\n|C|\n|Q|\n\nchanges to\n\n|S|C|Q|\n|--|--|--|\n|1|0|0|\n|0|1|0|\n|0|0|1|\n\n\n* <b>Remove columns deemed as uneccessary</b>","metadata":{}},{"cell_type":"code","source":"print(train_data.isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2022-01-15T19:19:27.198218Z","iopub.execute_input":"2022-01-15T19:19:27.198717Z","iopub.status.idle":"2022-01-15T19:19:27.206837Z","shell.execute_reply.started":"2022-01-15T19:19:27.198667Z","shell.execute_reply":"2022-01-15T19:19:27.206039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"check_missing_values\"></a>\n## Checking Missing Values","metadata":{}},{"cell_type":"markdown","source":"<b>Correting the *'Age'* column </b>\n<br>\nReplace the missing *'Age'* values with the <u>median</u> of all the present *'Age'* values.\n<br>\n\n<b>Why the median?</b>\n\nBelow, a histrogram is plotted showing the distribution of ages of passengers. From the graph, it can be seen that the data is right-skewed, meaning that the distribution has a long right tail. In this case, it is better to use the median of the data over the mean, as the mean is affected more by extreme/outlier data, or when data is skewed. Below, the mean age is higher than the median age, as can be seen with the 2 plotted vertical lines. Thus, the missing values will be replaced with the median of the values in the *'Age'* column.","metadata":{}},{"cell_type":"code","source":"sns.set(rc={\"figure.figsize\":(15,5)}) # set size of figure plotted\nsns.set_style(\"dark\")\nsns.histplot(train_data[\"Age\"], kde=True, bins=20, color=\"teal\")\nplt.axvline(train_data[\"Age\"].median(), c=\"red\", label=\"Median Age: {:.1f}\".format(train_data[\"Age\"].median()))\nplt.axvline(train_data[\"Age\"].mean(), c=\"blue\", label=\"Mean Age: {:.1f}\".format(train_data[\"Age\"].mean()))\nplt.legend()\nplt.suptitle(\"Age of Passengers: Right Skewed\", fontsize=20)\nplt.xlabel(\"Age\")\nplt.ylabel(\"Count\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-15T19:19:27.208061Z","iopub.execute_input":"2022-01-15T19:19:27.20829Z","iopub.status.idle":"2022-01-15T19:19:27.590739Z","shell.execute_reply.started":"2022-01-15T19:19:27.208255Z","shell.execute_reply":"2022-01-15T19:19:27.589824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Replacing values in the *'Age'* column","metadata":{}},{"cell_type":"code","source":"train_data['Age'].fillna(train_data['Age'].median(), inplace=True)\n\n# Check if missing values have been filled for the 'Age' column\nassert(train_data['Age'].isna().sum() == 0)\nprint(\"Missing 'Age' values:\", train_data['Age'].isna().sum())","metadata":{"execution":{"iopub.status.busy":"2022-01-15T19:19:27.594547Z","iopub.execute_input":"2022-01-15T19:19:27.594777Z","iopub.status.idle":"2022-01-15T19:19:27.602458Z","shell.execute_reply.started":"2022-01-15T19:19:27.59475Z","shell.execute_reply":"2022-01-15T19:19:27.601634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>Correcting the *'Embarked'* column</b>\n<br>\nDelete the rows with missing values in the *'Embarked'* column (There are only 2 missing, so this should be OK)","metadata":{}},{"cell_type":"code","source":"train_data = train_data.loc[train_data['Embarked'].notna(), :]\n\n# Check if missing values have been filled for the 'Embarked' column\nassert(train_data['Embarked'].isna().sum() == 0)\nprint(\"Missing 'Embarked' values:\", train_data['Embarked'].isna().sum())","metadata":{"execution":{"iopub.status.busy":"2022-01-15T19:19:27.603639Z","iopub.execute_input":"2022-01-15T19:19:27.603878Z","iopub.status.idle":"2022-01-15T19:19:27.617749Z","shell.execute_reply.started":"2022-01-15T19:19:27.60385Z","shell.execute_reply":"2022-01-15T19:19:27.616942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>Correcting the *'Cabin'* column</b>\n<br>\nThere are too many missing values. It may be best to delete the column","metadata":{}},{"cell_type":"code","source":"train_data.drop(\"Cabin\", axis=1, inplace=True)\n\n# Check if 'Cabin' column has been deleted\nassert('Cabin' not in train_data)\nprint(\"'Cabin' column present:\", ('Cabin' in train_data))","metadata":{"execution":{"iopub.status.busy":"2022-01-15T19:19:27.619952Z","iopub.execute_input":"2022-01-15T19:19:27.620176Z","iopub.status.idle":"2022-01-15T19:19:27.627368Z","shell.execute_reply.started":"2022-01-15T19:19:27.620148Z","shell.execute_reply":"2022-01-15T19:19:27.626818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"unusual_values\"></a>\n## Checking Unusual/Invalid Values\n\n### Categorical data","metadata":{}},{"cell_type":"code","source":"train_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-15T19:19:27.628474Z","iopub.execute_input":"2022-01-15T19:19:27.628666Z","iopub.status.idle":"2022-01-15T19:19:27.648616Z","shell.execute_reply.started":"2022-01-15T19:19:27.628642Z","shell.execute_reply":"2022-01-15T19:19:27.647963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categorical_cols = [\"Survived\", \"Pclass\", \"Sex\", \"Embarked\"]\n\nfor col in categorical_cols:\n    print(col, \":\", train_data[col].unique())","metadata":{"execution":{"iopub.status.busy":"2022-01-15T19:19:27.649698Z","iopub.execute_input":"2022-01-15T19:19:27.649947Z","iopub.status.idle":"2022-01-15T19:19:27.663901Z","shell.execute_reply.started":"2022-01-15T19:19:27.649919Z","shell.execute_reply":"2022-01-15T19:19:27.663101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Numerical Data\n\n<b>Checking *'Age'* column</b>\n<br>\nWill look at the spread of *'Age'* values, and replace negative '*Age*' values with the median *'Age'* value. Negative ages are invalid.\n<br>\n\nFrom the previous histogram of *'Age'*, it is clear that there are no ages that are too big. To be safe, the largest age is checked","metadata":{}},{"cell_type":"code","source":"# replace negative ages with the median age\ntrain_data.loc[train_data[\"Age\"] < 0, \"Age\"] = train_data[\"Age\"].median()\n\n# assert that there are no negative ages\nassert((train_data[\"Age\"] > 0).all())","metadata":{"execution":{"iopub.status.busy":"2022-01-15T19:19:27.665317Z","iopub.execute_input":"2022-01-15T19:19:27.66555Z","iopub.status.idle":"2022-01-15T19:19:27.671647Z","shell.execute_reply.started":"2022-01-15T19:19:27.665523Z","shell.execute_reply":"2022-01-15T19:19:27.671066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>Checking *'SibSp'* and *'Parch*' columns</b>\n<br>\nWill look at the spread of the values of these columns, and replace negative values with 0","metadata":{}},{"cell_type":"code","source":"# replace negative values with 0\ntrain_data.loc[train_data[\"SibSp\"] < 0, \"SibSp\"] = 0\ntrain_data.loc[train_data[\"Parch\"] < 0, \"Parch\"] = 0\n\n# assert that there are no negative values\nassert((train_data[\"SibSp\"] >= 0).all())\nassert((train_data[\"Parch\"] >= 0).all())","metadata":{"execution":{"iopub.status.busy":"2022-01-15T19:19:27.672555Z","iopub.execute_input":"2022-01-15T19:19:27.673197Z","iopub.status.idle":"2022-01-15T19:19:27.684088Z","shell.execute_reply.started":"2022-01-15T19:19:27.673163Z","shell.execute_reply":"2022-01-15T19:19:27.683098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>Checking *'Fare'* column</b>\n<br>\nWill look at the spread of *'Fare'* values, and replace negative '*Fare*' values with the median *'Fare'* value. Negative ages are invalid","metadata":{}},{"cell_type":"code","source":"sns.set(rc={\"figure.figsize\":(15,5)}) # set size of figure plotted\nsns.set_style(\"dark\")\nsns.histplot(train_data[\"Fare\"], kde=True, bins=50, color=\"teal\")\nplt.axvline(train_data[\"Fare\"].median(), c=\"red\", label=\"Median Fare: {:.1f}\".format(train_data[\"Fare\"].median()))\nplt.axvline(train_data[\"Fare\"].mean(), c=\"blue\", label=\"Mean Fare: {:.1f}\".format(train_data[\"Fare\"].mean()))\nplt.legend()\nplt.suptitle(\"Fare paid by Passengers: Right Skewed\", fontsize=20)\nplt.xlabel(\"Fare\")\nplt.ylabel(\"Count\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-15T19:19:27.68539Z","iopub.execute_input":"2022-01-15T19:19:27.686144Z","iopub.status.idle":"2022-01-15T19:19:28.128569Z","shell.execute_reply.started":"2022-01-15T19:19:27.686101Z","shell.execute_reply":"2022-01-15T19:19:28.127917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From this histogram we can see that the data is strongly right-skewed. Let us take a closer look at the tail end of fare prices\n<br>\n\nBelow the 50 highest fare prices in the dataset are printed out. From this, it can be seen that out of the 819 observations, a very small number of them are very high. This could correspond to the cabin prices, or perhaps extra luxuries afforded to them by paying more.","metadata":{}},{"cell_type":"code","source":"#train_data[\"Fare\"].sort_values().tail(n=15)\n\nsurvive_fare_df = train_data.loc[:, [\"Survived\",\"Fare\"]].sort_values(\"Fare\", ascending=False)\nprint(survive_fare_df[\"Fare\"].head(n=50).to_numpy())","metadata":{"execution":{"iopub.status.busy":"2022-01-15T19:19:28.129774Z","iopub.execute_input":"2022-01-15T19:19:28.130176Z","iopub.status.idle":"2022-01-15T19:19:28.138098Z","shell.execute_reply.started":"2022-01-15T19:19:28.130126Z","shell.execute_reply":"2022-01-15T19:19:28.137407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These outlier values may have a significant impact on the implementation of our model. Let us create a plot to investigate if this influences the survivability of the passengers","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(1,2)\nsns.countplot(x=survive_fare_df[\"Survived\"][:50], ax=axes[0], palette=['#f75e5e',\"#5da0e8\"])\nsns.countplot(x=survive_fare_df[\"Survived\"][50:], ax=axes[1], palette=['#f75e5e',\"#5da0e8\"])\naxes[0].set_title(\"Top 50 most expensive Fares & Survival\")\naxes[1].set_title(\"Excluding Top 50 most expensive Fares & Survival\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-15T19:19:28.139403Z","iopub.execute_input":"2022-01-15T19:19:28.139651Z","iopub.status.idle":"2022-01-15T19:19:28.633609Z","shell.execute_reply.started":"2022-01-15T19:19:28.139616Z","shell.execute_reply":"2022-01-15T19:19:28.632771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From this investigation and the graphs plotted above, we can confirm that people who paid the most fares were more likely to survive\n#### given everything else is equal!\n\nThese results do not take into account other factors, but is an indication that at first glance <u>we will not be removing these outliers</u>","metadata":{}},{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"encode_data\"></a>\n## Encoding Categorical Data\n\nHere, we will encode the categorical attributes using the One-Hot Encoder.\n\n<b>Pclass</b>","metadata":{}},{"cell_type":"code","source":"enc = OneHotEncoder()\nres = enc.fit_transform(train_data[[\"Pclass\"]]).toarray()\nres = pd.DataFrame(res, columns=[\"1\", \"2\", \"3\"], dtype='int8')","metadata":{"execution":{"iopub.status.busy":"2022-01-15T19:19:28.635096Z","iopub.execute_input":"2022-01-15T19:19:28.635549Z","iopub.status.idle":"2022-01-15T19:19:28.644538Z","shell.execute_reply.started":"2022-01-15T19:19:28.635501Z","shell.execute_reply":"2022-01-15T19:19:28.643833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.merge(train_data, res, left_index=True, right_index=True)\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-15T19:19:28.645995Z","iopub.execute_input":"2022-01-15T19:19:28.646498Z","iopub.status.idle":"2022-01-15T19:19:28.677506Z","shell.execute_reply.started":"2022-01-15T19:19:28.64645Z","shell.execute_reply":"2022-01-15T19:19:28.676683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>Sex</b>\n\nNote: It may not be necessary to have both *'Male'* and *'Female'* columns, as a 0 in the *'Male'* column can only mean that the passenger is a female. We have kept both columns though, as there doesn't seem to be a great need to save on datasize.","metadata":{}},{"cell_type":"code","source":"res = enc.fit_transform(train_data[[\"Sex\"]]).toarray()\nres = pd.DataFrame(res, columns=[\"Female\", \"Male\"], dtype='int8')","metadata":{"execution":{"iopub.status.busy":"2022-01-15T19:19:28.678696Z","iopub.execute_input":"2022-01-15T19:19:28.678978Z","iopub.status.idle":"2022-01-15T19:19:28.692605Z","shell.execute_reply.started":"2022-01-15T19:19:28.678948Z","shell.execute_reply":"2022-01-15T19:19:28.691864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.merge(train_data, res, left_index=True, right_index=True)\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-15T19:19:28.693918Z","iopub.execute_input":"2022-01-15T19:19:28.694615Z","iopub.status.idle":"2022-01-15T19:19:28.722004Z","shell.execute_reply.started":"2022-01-15T19:19:28.694579Z","shell.execute_reply":"2022-01-15T19:19:28.721024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>Embarked</b>","metadata":{}},{"cell_type":"code","source":"res = enc.fit_transform(train_data[[\"Embarked\"]]).toarray()\nres = pd.DataFrame(res, columns=[\"C\", \"Q\", \"S\"], dtype='int8')","metadata":{"execution":{"iopub.status.busy":"2022-01-15T19:19:28.723426Z","iopub.execute_input":"2022-01-15T19:19:28.723787Z","iopub.status.idle":"2022-01-15T19:19:28.736677Z","shell.execute_reply.started":"2022-01-15T19:19:28.723734Z","shell.execute_reply":"2022-01-15T19:19:28.735885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.merge(train_data, res, left_index=True, right_index=True)\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-15T19:19:28.737969Z","iopub.execute_input":"2022-01-15T19:19:28.738936Z","iopub.status.idle":"2022-01-15T19:19:28.76566Z","shell.execute_reply.started":"2022-01-15T19:19:28.738888Z","shell.execute_reply":"2022-01-15T19:19:28.764976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"remove_cols\"></a>\n## Removing Columns/Attributes\n\nFrom the training data, it appears that *'PassengerId'*, *'Name'* and *'Ticket'* will provide little use to the model, as it is difficult to make sense to how they relate to who survived. These columns will be removed","metadata":{}},{"cell_type":"markdown","source":"<b>Remove category columns</b>","metadata":{}},{"cell_type":"code","source":"train_data.drop([\"PassengerId\", \"Name\", \"Ticket\"], axis=1, inplace=True)\n\nassert(\"PassengerId\" not in train_data)\nassert(\"Name\" not in train_data)\nassert(\"Ticket\" not in train_data)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T19:19:28.766949Z","iopub.execute_input":"2022-01-15T19:19:28.767314Z","iopub.status.idle":"2022-01-15T19:19:28.774465Z","shell.execute_reply.started":"2022-01-15T19:19:28.767273Z","shell.execute_reply":"2022-01-15T19:19:28.773581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-15T19:19:28.77577Z","iopub.execute_input":"2022-01-15T19:19:28.776039Z","iopub.status.idle":"2022-01-15T19:19:28.804642Z","shell.execute_reply.started":"2022-01-15T19:19:28.776009Z","shell.execute_reply":"2022-01-15T19:19:28.803748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The *'SibSp'* and *'Parch'* columns can be merged together. We can also create another column representing whether the passenger was alone or not","metadata":{}},{"cell_type":"code","source":"train_data[\"Family\"] = train_data[\"SibSp\"] + train_data[\"Parch\"]\ntrain_data.drop([\"SibSp\", \"Parch\"], axis=1, inplace=True)\n\ntrain_data[\"Alone\"] = (~(train_data[\"Family\"] > 0)).astype(int) # alone is true if family > 0\n\nassert(\"SibSp\" not in train_data)\nassert(\"Parch\" not in train_data)\nassert(\"Alone\" in train_data)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T19:19:28.808596Z","iopub.execute_input":"2022-01-15T19:19:28.808883Z","iopub.status.idle":"2022-01-15T19:19:28.820659Z","shell.execute_reply.started":"2022-01-15T19:19:28.808849Z","shell.execute_reply":"2022-01-15T19:19:28.81962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-15T19:19:28.823877Z","iopub.execute_input":"2022-01-15T19:19:28.824203Z","iopub.status.idle":"2022-01-15T19:19:28.847155Z","shell.execute_reply.started":"2022-01-15T19:19:28.824168Z","shell.execute_reply":"2022-01-15T19:19:28.846218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Removing *'Pclass'*, *'Sex'* and *'Embarked'* as they have already been processed using the OneHotEncoder and are no longer needed","metadata":{}},{"cell_type":"code","source":"train_data.drop([\"Pclass\", \"Sex\", \"Embarked\"], axis=1, inplace=True)\n\nassert(\"Pclass\" not in train_data)\nassert(\"Sex\" not in train_data)\nassert(\"Embarked\" not in train_data)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T19:19:28.848466Z","iopub.execute_input":"2022-01-15T19:19:28.849049Z","iopub.status.idle":"2022-01-15T19:19:28.860415Z","shell.execute_reply.started":"2022-01-15T19:19:28.848997Z","shell.execute_reply":"2022-01-15T19:19:28.859581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-15T19:19:28.861787Z","iopub.execute_input":"2022-01-15T19:19:28.86244Z","iopub.status.idle":"2022-01-15T19:19:28.885067Z","shell.execute_reply.started":"2022-01-15T19:19:28.862408Z","shell.execute_reply":"2022-01-15T19:19:28.883948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"no_sklearn\"></a>\n# <p style=\"background-color:#0d101c;font-family:arial;color:#ffffff;font-size:150%;text-align:center;border-radius:20px;\">Model Training: From Scratch</p>\n\n## How does a SVM work?\n\nA SVM tries to create a hyperplace in a multi-dimensional space that divides the 2 classes. The margin (distance between the 2 closest points to the hyperplane) is something that we also try to maximise. If we maximise the margin, then the points of the different classes will be far away from each other and the seperating hyperplane. This will increase the general accuracy, but there may be miss-classifications, as this approach allows for that.<br>\n\n<b>*Support Vectors*</b> are the datapoints that touch the positive and negative hyperplanes. In the picture below, there are 2 support vectors touching the positive hyperplane, and 1 touching the negative hyperplane<br>\n\nImage source: https://en.wikipedia.org/wiki/Support-vector_machine#/media/File:SVM_margin.png\n<img src=\"https://github.com/datastrider/titanic_svm/blob/main/svm_diagram.jpg?raw=true\" style=\"width:50%\" align=\"left\"/><br>\n","metadata":{}},{"cell_type":"markdown","source":"### Algorithm<br>\n\nFrom the diagram, we can see that one class should not cross the line $w \\cdot x - b = 1$, whereas the other class should not cross the line $w \\cdot x - b = -1$ <br>\n\nFor this classifier, $y_{i}$ has to be either -1 or 1\n\nif $y_{i} = 1$, then $w \\cdot x_{i} - b >= 1$ <br>\nif $y_{i} = -1$, then $w \\cdot x_{i} - b <= -1$\n\nThe goal of the classifier is also to maximise the distance between the data points of the 2 classes.\n\n### Cost Function<br>\n\nIn order to know how well the classifier does, we need an algorithm that can produce a result based on the results of the predictions. For the SVM classifier, we will use a Hinge Loss Function\n\nHinge Loss: $l(y) = max(0, 1-y_{i}(w \\cdot x_{i} - b))$\n\nThis will return $0$ if $y >= 1$, otherwise $1-y_{i}(w \\cdot x_{i} - b)$. <br>\n\nThis means that correct predictions do not increase the loss function, only incorrect predictions. <br>\n\n### Regularisation\n\nIt is also important to consider regularisation. This takes the form of <br>\n\n$\\lambda ||w||^2 $ <br>\n\nThus, the function we will want to optimise using gradient descent is: <br>\n\n$J(\\theta) = \\frac{\\lambda}{2}||w||^2 + \\frac{1}{n} \\sum_{i=1}^{n} max(0, 1-y_{i}(w \\cdot x_{i} - b))$ <br>\n\nThe equation above is the <b>*Primal Form*</b> of the SVM. <br>\n\nMaking $\\lambda$ smaller makes the distance between the positive and negative hyperplanes larger. This leads to a hyperplane being drawn that seperates the data, with data representing the 2 different classes being far apart. Using the Hinge Loss function, and a smaller $\\lambda$ with a SVM is known as a <b>*Soft Margin*</b>, because it allows for miss-clasifications.\n_______________________________________\n\nRegularisation is explained here, with visualisations: https://datascience.stackexchange.com/questions/4943/intuition-for-the-regularization-parameter-in-svm. As lambda tends to infinity, the solution tends to a <b>*Hard Margin*</b>, where no miss-classifications are allowed.\n\nIt is, that $C \\sim \\frac{1}{\\lambda}$, and that it depends on the formulation of the SVM (https://stats.stackexchange.com/a/298886). We are not using the equation with C, but it achieves the same goal. The C version: <br>\n\n$\\frac{1}{2}||w||^2 + C \\frac{1}{n}\\sum_{i=1}^{n} max(0, 1-y_{i}(w \\cdot x_{i} - b))$\n\nThe 'C' parameter is explained very well here:<br>\nhttps://stats.stackexchange.com/a/159051<br>\nhttps://medium.com/@kushaldps1996/a-complete-guide-to-support-vector-machines-svms-501e71aec19e<br>\n\n### Gradient Descent\n\nAs we are trying to minimuse the loss function, we will be using gradient descent. We are trying to find the optimal values for $w$ and $b$, so we calculate the derivatives of the loss function with respect to each variable. There are 2 cases to take into account. <br>\n\n$y_{i}(w \\cdot x_{i} - b) >= 1$ and $y_{i}(w \\cdot x_{i} - b) < 1$<br>\n\nlet $h()$ be the Hinge Loss function with regularisation, then... <br>\n________________________________\nif $y_{i}(w \\cdot x_{i} - b) >= 1$ <br>\n\n$\\frac{\\partial J}{\\partial w} = 2 \\lambda w$\n\nAnd\n\n$\\frac{\\partial J}{\\partial b} = 0$\n\n_________________________________\n\nif $y_{i}(w \\cdot x_{i} - b) < 1$ <br>\n\n$\\frac{\\partial J}{\\partial w} = 2 \\lambda w - \\frac{1}{n}\\sum_{i=1}^{n} x_{i} \\cdot y_{i}$\n\nAnd\n\n$\\frac{\\partial J}{\\partial b} = \\frac{1}{n}\\sum_{i=1}^{n} y_{i}$\n\n_______________________________\n### Update Value Rules\n\n$w = w - \\alpha \\cdot dw$<br>\n$b = b - \\alpha \\cdot db$ <br>\n\nwhere $\\alpha$ is the learning rate\n________________________________\n\nBefore applying the learning rate to $dw$ and $db$, we need to calculate the summation values first: <br>\n\n$dw$: $\\frac{1}{n}\\sum_{i=1}^{n} x_{i} \\cdot y_{i}$ <br>\n\n$db$: $\\frac{1}{n}\\sum_{i=1}^{n}y_{i}$\n\nOnly after this, do you add $2 \\lambda w$ to $dw$ and then apply $\\alpha$, as shown in the update rules\n\nAfter each iteration, $w$ and $b$ will change, with influence from $\\alpha$ and $\\lambda$ constants, until the gradient descent settles in a local minima.\n\nIt is possible when using gradient descent to not get the optimal values for $w$ and $b$, as the algorithm can get stuck in a local minima. This happens when there are more than 1 minimas. There are ways to handle this, but it is not investigated in this project.<br>\n\nImage source: https://en.wikipedia.org/wiki/Maxima_and_minima#/media/File:Extrema_example_original.svg\n<img src=\"https://github.com/datastrider/titanic_svm/blob/main/minimas.jpg?raw=true\" style=\"width:50%\" align=\"left\"/><br>","metadata":{}},{"cell_type":"code","source":"class SVMClassifier:\n\n    def __init__(self, lam=0.01, lr=0.001, n_iter=1000):\n\n        self.lr = lr # learning rate\n        self.n_iter = n_iter # number of iterations\n        self.lam = lam\n        self.w = 0 # weights of attributes of X\n        self.b = 0 # bias\n        self.loss_history = []\n\n    def _hinge_loss(self, X: np.array, y: np.array, w: np.array, b: float):\n        '''\n        Returns the hinge loss of  a linear equation\n\n        #param X: observed datapoints\n        @param y: classification of corresponding x observations\n        @param w: weights of the linear equation\n        @param b: bias\n        @return: sum of losses\n        '''\n\n        loss = [max(0, 1-y_ * (np.dot(w, x_.T) - b)) for y_, x_ in zip(y, X)]\n\n        return np.sum(loss)\n\n    def fit(self, X: np.array, y: np.array):\n        '''\n        Fits the model using the data, making use of the\n        Hinge Loss Function and gradient descent to find the\n        values of 'w' and 'b'\n        \n        @param X: observed datapoints\n        @param y: classification of corresponding x observations\n        @return:\n        '''\n\n        y_temp = np.where(y == 0, -1, 1)\n        n_samples = X.shape[0]\n        n_features = X.shape[1]\n\n        self.w = np.zeros(n_features)\n        self.b = 0\n\n        for _ in range(self.n_iter):\n\n            loss = self._hinge_loss(X, y, self.w, self.b)\n            self.loss_history.append(loss)\n\n            # both used for summing values of corresponding derivatives\n            dw = np.zeros(n_features)\n            db = 0\n\n            for i, x_temp in enumerate(X):\n                \n                if (y_temp[i] * np.dot(x_temp, self.w) - self.b) >= 1:\n                    pass\n                    # hinge function returns 0\n                else:\n                    # summing values within the Sigma/Sum symbol\n                    dw += -np.dot(x_temp, y_temp[i]) #summation part of derivative\n                    db += self.lr * y_temp[i] # summation part of derivative\n\n                # adding lambda where appropriate and applying learning rate\n                self.w -= self.lr * (self.lam * self.w * 2 + dw/n_samples) # dw * 1/n as in the equation\n                self.b -= self.lr * db\n\n\n\n    def predict(self, X: np.array):\n        '''\n        Makes predictions on given datapoints and returns them\n        as either 0 or 1\n        \n        @param X: observed datapoints\n        @return pred: predictions\n        '''\n        \n        pred = np.dot(X, self.w) - self.b\n        pred = np.sign(pred)\n        pred = np.where(pred < 0, 0, 1)\n        \n        return pred\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-15T19:19:28.886895Z","iopub.execute_input":"2022-01-15T19:19:28.887162Z","iopub.status.idle":"2022-01-15T19:19:28.90429Z","shell.execute_reply.started":"2022-01-15T19:19:28.887129Z","shell.execute_reply":"2022-01-15T19:19:28.903554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(train_data.iloc[:, 1:].to_numpy(),\n                                                    train_data.iloc[:, 0].to_numpy(),\n                                                    random_state=123,\n                                                    train_size=0.8)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T19:19:28.906006Z","iopub.execute_input":"2022-01-15T19:19:28.906908Z","iopub.status.idle":"2022-01-15T19:19:28.924076Z","shell.execute_reply.started":"2022-01-15T19:19:28.906852Z","shell.execute_reply":"2022-01-15T19:19:28.923001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = SVMClassifier()\n\nclf.fit(X_train, y_train)\npred = clf.predict(X_test)\n\nprint(\"Accuracy:\", accuracy_score(y_test, pred))","metadata":{"execution":{"iopub.status.busy":"2022-01-15T19:19:28.925629Z","iopub.execute_input":"2022-01-15T19:19:28.925935Z","iopub.status.idle":"2022-01-15T19:19:52.411955Z","shell.execute_reply.started":"2022-01-15T19:19:28.925889Z","shell.execute_reply":"2022-01-15T19:19:52.41102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Our SVM Classifier Results\n\nFrom this simple implementation, with no optimisation, we can see that it scores closely with the sklearn implementation that we will see in the next [section](#sklearn), where we implement and optimise the sklearn implementation for the Kaggle submission.\n\nBelow is a graph that shows the improvement of the loss function during the fitting/training of our SVM Classifier. It does seem to be erratic at times (this can be fixed by doing multiple runs and taking an average), but it is clear that there is a downward trend in the loss function, meaning our SVM Classifier is improving.","metadata":{}},{"cell_type":"code","source":"plt.figure()\nplt.plot(clf.loss_history)\nplt.title(\"Loss Hisotry of Training Own SVM Classifier\")\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-15T19:19:52.413277Z","iopub.execute_input":"2022-01-15T19:19:52.413488Z","iopub.status.idle":"2022-01-15T19:19:52.644386Z","shell.execute_reply.started":"2022-01-15T19:19:52.413462Z","shell.execute_reply":"2022-01-15T19:19:52.643837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"sklearn\"></a>\n# <p style=\"background-color:#0d101c;font-family:arial;color:#ffffff;font-size:150%;text-align:center;border-radius:20px;\">Model Training: Sklearn</p>","metadata":{}},{"cell_type":"code","source":"random_state = 123\nX_train, X_test, y_train, y_test = train_test_split(train_data.iloc[:, 1:], # X\n                                                    train_data.iloc[:, 0], # y\n                                                    random_state=random_state,\n                                                    train_size=0.8)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T19:19:52.645509Z","iopub.execute_input":"2022-01-15T19:19:52.645933Z","iopub.status.idle":"2022-01-15T19:19:52.652229Z","shell.execute_reply.started":"2022-01-15T19:19:52.645891Z","shell.execute_reply":"2022-01-15T19:19:52.651752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = SVC()\n\nclf.fit(X_train, y_train)\n\npred = clf.predict(X_test)\n\nprint(\"Accuracy: \", accuracy_score(y_test, pred))","metadata":{"execution":{"iopub.status.busy":"2022-01-15T19:19:52.653223Z","iopub.execute_input":"2022-01-15T19:19:52.653598Z","iopub.status.idle":"2022-01-15T19:19:52.692382Z","shell.execute_reply.started":"2022-01-15T19:19:52.65357Z","shell.execute_reply":"2022-01-15T19:19:52.691631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the SVM Classifier coded from scratch is performing better than the Sklearn SVC version on the same train/test data and random_seed. <br> Perhaps this will soon change. We will now try to optimise the hyper-parameters.","metadata":{}},{"cell_type":"code","source":"params = {\"C\": (0.1, 2),\n          \"kernel\": [\"linear\", \"poly\", \"sigmoid\"],\n          \"degree\": (1, 3),\n          \"coef0\": (0.0, 1.0)}\n\nopt = BayesSearchCV(clf,\n                    params,\n                    cv=10,\n                    random_state=random_state)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T19:25:39.04978Z","iopub.execute_input":"2022-01-15T19:25:39.050162Z","iopub.status.idle":"2022-01-15T19:25:39.060929Z","shell.execute_reply.started":"2022-01-15T19:25:39.050122Z","shell.execute_reply":"2022-01-15T19:25:39.059728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"opt.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T19:25:41.289298Z","iopub.execute_input":"2022-01-15T19:25:41.289586Z","iopub.status.idle":"2022-01-15T19:28:30.39909Z","shell.execute_reply.started":"2022-01-15T19:25:41.289557Z","shell.execute_reply":"2022-01-15T19:28:30.398217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Accuracy:\",opt.score(X_test, y_test))","metadata":{"execution":{"iopub.status.busy":"2022-01-15T19:28:39.123472Z","iopub.execute_input":"2022-01-15T19:28:39.124025Z","iopub.status.idle":"2022-01-15T19:28:39.133381Z","shell.execute_reply.started":"2022-01-15T19:28:39.123967Z","shell.execute_reply":"2022-01-15T19:28:39.132274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"opt.best_params_","metadata":{"execution":{"iopub.status.busy":"2022-01-15T19:29:42.698347Z","iopub.execute_input":"2022-01-15T19:29:42.698721Z","iopub.status.idle":"2022-01-15T19:29:42.706005Z","shell.execute_reply.started":"2022-01-15T19:29:42.698666Z","shell.execute_reply":"2022-01-15T19:29:42.705101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"kaggle_sub\"></a>\n# <p style=\"background-color:#0d101c;font-family:arial;color:#ffffff;font-size:150%;text-align:center;border-radius:20px;\">Kaggle Submission</p>","metadata":{}},{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"clean_test_data\"></a>\n## Cleaning and Processing Test Data\n\nWe will process the test data to bring it in line with the training data used.\n<br>\nAll steps carried out below were carried out on the training data above.","metadata":{}},{"cell_type":"code","source":"test_id = test_data[\"PassengerId\"]\n\ntest_data['Age'].fillna(test_data['Age'].median(), inplace=True)\ntest_data[\"Fare\"].fillna(test_data[\"Fare\"].median(), inplace=True)\n\n# Check if missing values have been filled for the 'Age' column\nassert(test_data['Age'].isna().sum() == 0)\n\ntest_data = test_data.loc[test_data['Embarked'].notna(), :]\n\n# Check if missing values have been filled for the 'Embarked' column\nassert(test_data['Embarked'].isna().sum() == 0)\n\ntest_data.drop(\"Cabin\", axis=1, inplace=True)\n\n# Check if 'Cabin' column has been deleted\nassert('Cabin' not in test_data)\n\ncategorical_cols = [\"Pclass\", \"Sex\", \"Embarked\"]\n\n# replace negative ages with the median age\ntest_data.loc[test_data[\"Age\"] < 0, \"Age\"] = test_data[\"Age\"].median()\n\n# assert that there are no negative ages\nassert((test_data[\"Age\"] > 0).all())\n\n# replace negative values with 0\ntest_data.loc[test_data[\"SibSp\"] < 0, \"SibSp\"] = 0\ntest_data.loc[test_data[\"Parch\"] < 0, \"Parch\"] = 0\n\n# assert that there are no negative values\nassert((test_data[\"SibSp\"] >= 0).all())\nassert((test_data[\"Parch\"] >= 0).all())\n\nenc = OneHotEncoder()\nres = enc.fit_transform(test_data[[\"Pclass\"]]).toarray()\nres = pd.DataFrame(res, columns=[\"1\", \"2\", \"3\"], dtype='int8')\ntest_data = pd.merge(test_data, res, left_index=True, right_index=True)\n\nres = enc.fit_transform(test_data[[\"Sex\"]]).toarray()\nres = pd.DataFrame(res, columns=[\"Female\", \"Male\"], dtype='int8')\ntest_data = pd.merge(test_data, res, left_index=True, right_index=True)\n\nres = enc.fit_transform(test_data[[\"Embarked\"]]).toarray()\nres = pd.DataFrame(res, columns=[\"C\", \"Q\", \"S\"], dtype='int8')\ntest_data = pd.merge(test_data, res, left_index=True, right_index=True)\n\ntest_data.drop([\"PassengerId\", \"Name\", \"Ticket\"], axis=1, inplace=True)\n\nassert(\"PassengerId\" not in test_data)\nassert(\"Name\" not in test_data)\nassert(\"Ticket\" not in test_data)\n\ntest_data[\"Family\"] = test_data[\"SibSp\"] + test_data[\"Parch\"]\ntest_data.drop([\"SibSp\", \"Parch\"], axis=1, inplace=True)\n\ntest_data[\"Alone\"] = (~(test_data[\"Family\"] > 0)).astype(int) # alone is true if family > 0\n\nassert(\"SibSp\" not in test_data)\nassert(\"Parch\" not in test_data)\nassert(\"Alone\" in test_data)\n\ntest_data.drop([\"Pclass\", \"Sex\", \"Embarked\"], axis=1, inplace=True)\n\nassert(\"Pclass\" not in test_data)\nassert(\"Sex\" not in test_data)\nassert(\"Embarked\" not in test_data)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T19:20:32.89902Z","iopub.status.idle":"2022-01-15T19:20:32.899334Z","shell.execute_reply.started":"2022-01-15T19:20:32.899155Z","shell.execute_reply":"2022-01-15T19:20:32.899171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"create_submission_csv\"></a>\n## Create sumbission csv","metadata":{}},{"cell_type":"code","source":"clf = SVC()\nclf.set_params(**opt.best_params_)\n\nclf.fit(train_data.iloc[:,1:].to_numpy(), # X\n        train_data.iloc[:,0].to_numpy()) # y\n\npred = clf.predict(X_test)\nprint(pred)\n\nsubmit = pd.merge(test_id, pd.DataFrame(pred, columns=[\"Survived\"]), left_index=True, right_index=True)\nsubmit.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T19:20:32.90026Z","iopub.status.idle":"2022-01-15T19:20:32.900559Z","shell.execute_reply.started":"2022-01-15T19:20:32.900403Z","shell.execute_reply":"2022-01-15T19:20:32.900418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Thanks for Viewing","metadata":{}}]}